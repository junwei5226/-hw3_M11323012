# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10d877AzKb1tbWr-WyLbJc-JwcGItl28P
"""

# 導入基本函式庫
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import cv2
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard
from google.colab import files
import zipfile

# 檢查環境
print("TensorFlow版本:", tf.__version__)
print("GPU可用:", len(tf.config.list_physical_devices('GPU')) > 0)

# 設定隨機種子確保實驗可重現
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)

# 建立結果目錄
!mkdir -p /content/results/fold1

# 請先將Fold1從本地上傳到Colab
print("請上傳Fold1資料夾的ZIP檔案...")
uploaded = files.upload()  # 上傳Fold1壓縮檔

# 解壓縮上傳的文件
for filename in uploaded.keys():
    print(f'解壓縮 {filename}...')
    with zipfile.ZipFile(filename, 'r') as zip_ref:
        zip_ref.extractall('/content')
    print('解壓縮完成!')

# 設定資料路徑
data_path = '/content/Fold1'
train_img_path = os.path.join(data_path, 'train')
train_mask_path = os.path.join(data_path, 'trainannot')
val_img_path = os.path.join(data_path, 'val')
val_mask_path = os.path.join(data_path, 'valannot')
test_img_path = os.path.join(data_path, 'test')
test_mask_path = os.path.join(data_path, 'testannot')

# 資料載入函數
def load_images(img_dir, mask_dir, img_size=(256, 256)):
    images = []
    masks = []
    img_files = sorted(os.listdir(img_dir))
    mask_files = sorted(os.listdir(mask_dir))

    print(f"讀取資料夾: {img_dir} 中的 {len(img_files)} 個檔案")

    for img_file, mask_file in zip(img_files, mask_files):
        # 讀取圖像和遮罩
        img_path = os.path.join(img_dir, img_file)
        mask_path = os.path.join(mask_dir, mask_file)

        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

        # 調整大小
        if img is not None and mask is not None:
            img = cv2.resize(img, img_size)
            mask = cv2.resize(mask, img_size)

            # 規範化
            img = img / 255.0
            mask = (mask > 0).astype(np.float32)  # 二值化遮罩

            # 添加通道維度
            img = np.expand_dims(img, axis=-1)
            mask = np.expand_dims(mask, axis=-1)

            images.append(img)
            masks.append(mask)
        else:
            print(f"警告: 無法讀取 {img_path} 或 {mask_path}")

    return np.array(images), np.array(masks)

def data_augmentation(image, mask):
    """資料增強: 對比度增強、隨機縮放、亮度調整"""
    # 確保拷貝而不是直接修改原始數據
    image = image.copy()
    mask = mask.copy()

    # 保存原始形狀以便後續恢復
    original_shape = image.shape

    # 去除通道維度便於處理
    if len(image.shape) == 3 and image.shape[-1] == 1:
        image = image[:, :, 0]
    if len(mask.shape) == 3 and mask.shape[-1] == 1:
        mask = mask[:, :, 0]

    # 對比度增強
    if np.random.random() > 0.5:
        alpha = np.random.uniform(0.8, 1.5)  # 對比度因子
        mean = np.mean(image)
        image = mean + alpha * (image - mean)
        image = np.clip(image, 0, 1)

    # 隨機縮放
    if np.random.random() > 0.5:
        scale = np.random.uniform(0.9, 1.1)
        h, w = image.shape
        new_h, new_w = int(h * scale), int(w * scale)

        # 執行縮放
        image_resized = cv2.resize(image, (new_w, new_h))
        mask_resized = cv2.resize(mask, (new_w, new_h))

        # 處理縮放後大小不一致的情況
        if new_h != h or new_w != w:
            # 創建新的畫布
            new_image = np.zeros((h, w))
            new_mask = np.zeros((h, w))

            # 計算貼上位置 (居中)
            y_offset = max(0, (h - new_h) // 2)
            x_offset = max(0, (w - new_w) // 2)

            # 計算裁剪區域 (如果放大了)
            y_crop = max(0, (new_h - h) // 2)
            x_crop = max(0, (new_w - w) // 2)
            crop_h = min(new_h, h)
            crop_w = min(new_w, w)

            # 將縮放後的圖像放入新畫布
            if new_h <= h and new_w <= w:
                # 如果縮小了，直接貼上
                new_image[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = image_resized
                new_mask[y_offset:y_offset+new_h, x_offset:x_offset+new_w] = mask_resized
            else:
                # 如果放大了，需要裁剪
                new_image = image_resized[y_crop:y_crop+crop_h, x_crop:x_crop+crop_w]
                new_mask = mask_resized[y_crop:y_crop+crop_h, x_crop:x_crop+crop_w]

                # 如果裁剪後尺寸不一致，再次調整大小
                if new_image.shape != (h, w):
                    new_image = cv2.resize(new_image, (w, h))
                    new_mask = cv2.resize(new_mask, (w, h))

            image = new_image
            mask = new_mask

    # 亮度調整
    if np.random.random() > 0.5:
        factor = np.random.uniform(0.7, 1.3)
        image = np.clip(image * factor, 0, 1)

    # 重新添加通道維度以匹配原始形狀
    if len(original_shape) == 3 and original_shape[-1] == 1:
        image = np.expand_dims(image, axis=-1)
        mask = np.expand_dims(mask, axis=-1)

    # 確保最終形狀與原始形狀一致
    assert image.shape == original_shape, f"增強後圖像形狀 {image.shape} 與原始形狀 {original_shape} 不符"
    assert mask.shape == original_shape, f"增強後遮罩形狀 {mask.shape} 與原始形狀 {original_shape} 不符"

    return image, mask

# 圖像資料產生器
class ImageDataGenerator(keras.utils.Sequence):
    def __init__(self, images, masks, batch_size=8, augment=False):
        self.images = images
        self.masks = masks
        self.batch_size = batch_size
        self.augment = augment
        self.indexes = np.arange(len(images))
        np.random.shuffle(self.indexes)

    def __len__(self):
        return int(np.ceil(len(self.images) / self.batch_size))

    def __getitem__(self, index):
        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        batch_images = np.array([self.images[i] for i in batch_indexes])
        batch_masks = np.array([self.masks[i] for i in batch_indexes])

        if self.augment:
            # 資料增強
            for i in range(len(batch_images)):
                batch_images[i], batch_masks[i] = data_augmentation(batch_images[i], batch_masks[i])

        return batch_images, batch_masks

    def on_epoch_end(self):
        np.random.shuffle(self.indexes)

# 載入資料
print("載入訓練和驗證資料...")
train_images, train_masks = load_images(train_img_path, train_mask_path)
val_images, val_masks = load_images(val_img_path, val_mask_path)
test_images, test_masks = load_images(test_img_path, test_mask_path)

print(f"訓練資料: {train_images.shape} - {train_masks.shape}")
print(f"驗證資料: {val_images.shape} - {val_masks.shape}")
print(f"測試資料: {test_images.shape} - {test_masks.shape}")

# 建立資料產生器
train_gen = ImageDataGenerator(train_images, train_masks, batch_size=8, augment=True)
val_gen = ImageDataGenerator(val_images, val_masks, batch_size=8, augment=False)

# 自定義端點檢測和評估指標函數
def find_endpoint(mask, pixel_per_cm=72):
    """找出遮罩中的ETT端點（最下方的點）"""
    if np.sum(mask) == 0:  # 如果遮罩為空
        return None

    # 找出遮罩中的所有點
    points = np.where(mask > 0.5)
    if len(points[0]) == 0:
        return None

    # 找出y座標最大的點（最下方的點）
    endpoint_idx = np.argmax(points[0])
    y_endpoint = points[0][endpoint_idx]

    return y_endpoint

def mean_error_cm(y_true, y_pred, pixel_per_cm=72):
    """計算平均誤差公分"""
    total_error = 0
    count = 0

    for i in range(len(y_true)):
        true_endpoint = find_endpoint(y_true[i,:,:,0])
        pred_endpoint = find_endpoint(y_pred[i,:,:,0] > 0.5)

        if true_endpoint is not None and pred_endpoint is not None:
            # 計算像素誤差
            error_pixels = abs(true_endpoint - pred_endpoint)
            # 轉換為公分
            error_cm = error_pixels / pixel_per_cm
            total_error += error_cm
            count += 1

    if count == 0:
        return float('inf')

    return total_error / count

def accuracy_within_threshold(y_true, y_pred, threshold_cm, pixel_per_cm=72):
    """計算誤差在threshold_cm公分內的準確率"""
    threshold_pixels = threshold_cm * pixel_per_cm
    correct_count = 0
    total_count = 0

    for i in range(len(y_true)):
        true_endpoint = find_endpoint(y_true[i,:,:,0])
        pred_endpoint = find_endpoint(y_pred[i,:,:,0] > 0.5)

        if true_endpoint is not None and pred_endpoint is not None:
            # 計算像素誤差
            error_pixels = abs(true_endpoint - pred_endpoint)
            if error_pixels <= threshold_pixels:
                correct_count += 1
            total_count += 1

    if total_count == 0:
        return 0.0

    return (correct_count / total_count) * 100

# 自定義IoU指標
def iou_score(y_true, y_pred, smooth=1e-6, threshold=0.5):
    y_pred_binary = tf.cast(y_pred > threshold, tf.float32)
    intersection = tf.reduce_sum(y_true * y_pred_binary, axis=[1, 2, 3])
    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred_binary, axis=[1, 2, 3]) - intersection
    iou = (intersection + smooth) / (union + smooth)
    return tf.reduce_mean(iou)

# 自定義Dice損失函數
def dice_loss(y_true, y_pred, smooth=1e-6):
    y_true_f = tf.reshape(y_true, [-1])
    y_pred_f = tf.reshape(y_pred, [-1])
    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    return 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)

# 自定義Focal Loss
def binary_focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):
    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)
    loss = - (alpha * y_true * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred) +
             (1 - alpha) * (1 - y_true) * tf.pow(y_pred, gamma) * tf.math.log(1 - y_pred))
    return tf.reduce_mean(loss)

# 自定義評估回調函數
class ETTEndpointMetrics(tf.keras.callbacks.Callback):
    def __init__(self, validation_data, pixel_per_cm=72):
        super(ETTEndpointMetrics, self).__init__()
        self.validation_data = validation_data
        self.pixel_per_cm = pixel_per_cm
        self.metrics_history = {
            'val_mean_error_cm': [],
            'val_accuracy_0.5cm': [],
            'val_accuracy_1.0cm': []
        }

    def on_epoch_end(self, epoch, logs=None):
        # 獲取驗證資料
        val_images, val_masks = self.validation_data

        # 預測
        val_preds = self.model.predict(val_images)

        # 計算端點指標
        mean_err = mean_error_cm(val_masks, val_preds, self.pixel_per_cm)
        acc_0_5 = accuracy_within_threshold(val_masks, val_preds, 0.5, self.pixel_per_cm)
        acc_1_0 = accuracy_within_threshold(val_masks, val_preds, 1.0, self.pixel_per_cm)

        # 更新logs
        logs = logs or {}
        logs['val_mean_error_cm'] = mean_err
        logs['val_accuracy_0.5cm'] = acc_0_5
        logs['val_accuracy_1.0cm'] = acc_1_0

        # 儲存指標歷史
        self.metrics_history['val_mean_error_cm'].append(mean_err)
        self.metrics_history['val_accuracy_0.5cm'].append(acc_0_5)
        self.metrics_history['val_accuracy_1.0cm'].append(acc_1_0)

        # 輸出指標
        print(f" - val_mean_error_cm: {mean_err:.4f} - val_accuracy_0.5cm: {acc_0_5:.2f}% - val_accuracy_1.0cm: {acc_1_0:.2f}%")

# 建立 Atrous Spatial Pyramid Pooling (ASPP) 模塊
def ASPP(inputs, dilations=[6, 12, 18]):
    # 圖像輸入的形狀
    _, h, w, c = inputs.shape

    # 全局平均池化分支
    global_avg_pool = layers.GlobalAveragePooling2D()(inputs)
    global_avg_pool = layers.Reshape((1, 1, c))(global_avg_pool)
    global_avg_pool = layers.Conv2D(256, kernel_size=1, padding='same', use_bias=False)(global_avg_pool)
    global_avg_pool = layers.BatchNormalization()(global_avg_pool)
    global_avg_pool = layers.Activation('relu')(global_avg_pool)
    global_avg_pool = layers.UpSampling2D(size=(h, w), interpolation='bilinear')(global_avg_pool)

    # 1x1 卷積分支
    conv_1x1 = layers.Conv2D(256, kernel_size=1, padding='same', use_bias=False)(inputs)
    conv_1x1 = layers.BatchNormalization()(conv_1x1)
    conv_1x1 = layers.Activation('relu')(conv_1x1)

    # 多個擴張卷積分支
    atrous_branches = []
    for dilation_rate in dilations:
        x = layers.Conv2D(256, kernel_size=3, padding='same',
                          dilation_rate=dilation_rate, use_bias=False)(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        atrous_branches.append(x)

    # 合併所有分支
    concat_branches = [conv_1x1, global_avg_pool] + atrous_branches
    x = layers.Concatenate(axis=-1)(concat_branches)

    # 輸出投影
    x = layers.Conv2D(256, kernel_size=1, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    return x

# 建立DeepLabV3+模型
def build_deeplabv3_plus_model(input_shape=(256, 256, 1)):
    # 輸入層
    inputs = keras.Input(shape=input_shape)

    # 編碼器部分（特徵提取）
    # 初始處理
    x = layers.Conv2D(64, 7, strides=2, padding='same')(inputs)  # 降為1/2: 128x128
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x1 = x  # 保存第一階段特徵 - 尺寸: 128x128

    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)  # 降為1/4: 64x64

    # ResNet塊 1
    skip_connection = x
    x = layers.Conv2D(64, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2D(64, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, skip_connection])
    x = layers.Activation('relu')(x)
    x2 = x  # 保存第二階段特徵 - 尺寸: 64x64

    # ResNet塊 2 - 下採樣
    skip_connection = layers.Conv2D(128, 1, strides=2, padding='same')(x)  # 降為1/8: 32x32
    skip_connection = layers.BatchNormalization()(skip_connection)

    x = layers.Conv2D(128, 3, strides=2, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2D(128, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, skip_connection])
    x = layers.Activation('relu')(x)

    # 保存低層特徵用於decoder
    low_level_features = x  # 尺寸: 32x32
    x3 = x  # 保存第三階段特徵

    # ResNet塊 3 - 下採樣
    skip_connection = layers.Conv2D(256, 1, strides=2, padding='same')(x)  # 降為1/16: 16x16
    skip_connection = layers.BatchNormalization()(skip_connection)

    x = layers.Conv2D(256, 3, strides=2, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2D(256, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, skip_connection])
    x = layers.Activation('relu')(x)
    x4 = x  # 保存第四階段特徵 - 尺寸: 16x16

    # ResNet塊 4 - 下採樣
    skip_connection = layers.Conv2D(512, 1, strides=2, padding='same')(x)  # 降為1/32: 8x8
    skip_connection = layers.BatchNormalization()(skip_connection)

    x = layers.Conv2D(512, 3, strides=2, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2D(512, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, skip_connection])
    x = layers.Activation('relu')(x)
    x5 = x  # 保存第五階段特徵 - 尺寸: 8x8

    # ASPP模塊
    aspp_features = ASPP(x5)  # 尺寸: 8x8

    # 解碼器部分 - 上採樣和特徵融合
    # 第一次上採樣到1/16大小
    x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(aspp_features)  # 從8x8到16x16

    # 融合第四階段特徵
    x = layers.Concatenate()([x, x4])
    x = layers.Conv2D(256, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    # 第二次上採樣到1/8大小
    x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)  # 從16x16到32x32

    # 融合第三階段特徵
    x = layers.Concatenate()([x, x3])
    x = layers.Conv2D(128, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    # 第三次上採樣到1/4大小
    x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)  # 從32x32到64x64

    # 融合第二階段特徵
    x = layers.Concatenate()([x, x2])
    x = layers.Conv2D(64, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    # 第四次上採樣到1/2大小
    x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)  # 從64x64到128x128

    # 融合第一階段特徵
    x = layers.Concatenate()([x, x1])
    x = layers.Conv2D(32, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    # 最終上採樣到原始尺寸
    x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)  # 從128x128到256x256

    # 額外的卷積層進一步完善特徵
    x = layers.Conv2D(16, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    # 輸出層 - 1個通道用於二值分割
    outputs = layers.Conv2D(1, 1, activation='sigmoid')(x)

    # 建立模型
    model = keras.Model(inputs, outputs)

    # 編譯模型
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-4),
        loss=dice_loss,
        metrics=[iou_score]
    )

    return model

# 建立模型
model = build_deeplabv3_plus_model(input_shape=(256, 256, 1))
model.summary()

# 設定回調函數
checkpoint = ModelCheckpoint(
    '/content/results/fold1/best_model.h5',
    monitor='val_iou_score',
    mode='max',
    save_best_only=True,
    verbose=1
)

early_stopping = EarlyStopping(
    monitor='val_iou_score',
    mode='max',
    patience=15,
    verbose=1
)

tensorboard = TensorBoard(
    log_dir='/content/results/fold1/logs',
    write_images=True
)

endpoint_metrics = ETTEndpointMetrics(
    validation_data=(val_images, val_masks),
    pixel_per_cm=72
)

# 訓練模型
print("開始訓練模型...")
EPOCHS = 20
BATCH_SIZE = 8

history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=EPOCHS,
    callbacks=[checkpoint, early_stopping, tensorboard, endpoint_metrics],
    verbose=1
)

# 載入最佳模型
best_model = keras.models.load_model(
    '/content/results/fold1/best_model.h5',
    custom_objects={
        'iou_score': iou_score,
        'dice_loss': dice_loss
    }
)

# 評估模型
print("評估測試集...")
test_preds = best_model.predict(test_images)
test_iou = iou_score(test_masks, test_preds).numpy()
test_mean_error = mean_error_cm(test_masks, test_preds)
test_acc_0_5 = accuracy_within_threshold(test_masks, test_preds, 0.5)
test_acc_1_0 = accuracy_within_threshold(test_masks, test_preds, 1.0)

# 儲存評估結果
results = {
    'fold': 1,
    'iou_score': float(test_iou),
    'mean_error_cm': test_mean_error,
    'accuracy_0.5cm': test_acc_0_5,
    'accuracy_1.0cm': test_acc_1_0
}

# 將結果轉換為DataFrame並儲存為CSV
results_df = pd.DataFrame([results])
results_df.to_csv('/content/results/fold1/metrics.csv', index=False)
print("測試集評估結果:")
print(f"IOU Score: {test_iou:.4f}")
print(f"Mean Error (cm): {test_mean_error:.4f}")
print(f"Accuracy within 0.5cm: {test_acc_0_5:.2f}%")
print(f"Accuracy within 1.0cm: {test_acc_1_0:.2f}%")

# 視覺化訓練歷史
plt.figure(figsize=(12, 4))
plt.subplot(1, 2, 1)
plt.plot(history.history['iou_score'])
plt.plot(history.history['val_iou_score'])
plt.title('Model IOU Score')
plt.ylabel('IOU')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.savefig('/content/results/fold1/training_history.png')
plt.show()

# 視覺化預測結果
def visualize_results(images, masks, predictions, num_samples=5):
    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples*5))

    for i in range(min(num_samples, len(images))):
        # 原始圖像
        axes[i, 0].imshow(images[i, :, :, 0], cmap='gray')
        axes[i, 0].set_title('原始圖像')
        axes[i, 0].axis('off')

        # 真實遮罩
        axes[i, 1].imshow(masks[i, :, :, 0], cmap='viridis')
        true_endpoint = find_endpoint(masks[i, :, :, 0])
        if true_endpoint is not None:
            axes[i, 1].scatter(images.shape[2]//2, true_endpoint, c='r', marker='x', s=100)
            axes[i, 1].text(images.shape[2]//2, true_endpoint+10, 'G', color='red', fontsize=12)
        axes[i, 1].set_title('真實遮罩')
        axes[i, 1].axis('off')

        # 預測遮罩
        axes[i, 2].imshow(predictions[i, :, :, 0], cmap='viridis')
        pred_endpoint = find_endpoint(predictions[i, :, :, 0] > 0.5)
        if pred_endpoint is not None:
            axes[i, 2].scatter(images.shape[2]//2, pred_endpoint, c='y', marker='x', s=100)
            axes[i, 2].text(images.shape[2]//2, pred_endpoint+10, 'Y', color='yellow', fontsize=12)
        axes[i, 2].set_title('預測遮罩')
        axes[i, 2].axis('off')

    plt.tight_layout()
    plt.savefig('/content/results/fold1/predictions.png')
    plt.show()

# 視覺化測試集結果
print("視覺化預測結果...")
sample_indices = np.random.choice(len(test_images), size=min(5, len(test_images)), replace=False)
visualize_results(
    test_images[sample_indices],
    test_masks[sample_indices],
    test_preds[sample_indices]
)

# 壓縮並下載結果
!zip -r /content/fold1_results.zip /content/results/fold1
files.download('/content/fold1_results.zip')

print("Fold1訓練完成！")