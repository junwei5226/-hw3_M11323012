# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B2df66CZ6j5ir-VdUxS_bcTf1vNSBwMh
"""

# 導入基本函式庫
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import cv2
from sklearn.model_selection import train_test_split
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau
from google.colab import files
import zipfile

# 檢查環境
print("TensorFlow版本:", tf.__version__)
print("GPU可用:", len(tf.config.list_physical_devices('GPU')) > 0)

# 設定隨機種子確保實驗可重現
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)

# 建立結果目錄
!mkdir -p /content/results/fold3

# 請先將Fold3從本地上傳到Colab
print("請上傳Fold3資料夾的ZIP檔案...")
uploaded = files.upload()  # 上傳Fold3壓縮檔

# 解壓縮上傳的文件
for filename in uploaded.keys():
    print(f'解壓縮 {filename}...')
    with zipfile.ZipFile(filename, 'r') as zip_ref:
        zip_ref.extractall('/content')
    print('解壓縮完成!')

# 設定資料路徑
data_path = '/content/Fold3'
train_img_path = os.path.join(data_path, 'train')
train_mask_path = os.path.join(data_path, 'trainannot')
val_img_path = os.path.join(data_path, 'val')
val_mask_path = os.path.join(data_path, 'valannot')
test_img_path = os.path.join(data_path, 'test')
test_mask_path = os.path.join(data_path, 'testannot')

# 資料載入函數
def load_images(img_dir, mask_dir, img_size=(256, 256)):
    images = []
    masks = []
    img_files = sorted(os.listdir(img_dir))
    mask_files = sorted(os.listdir(mask_dir))

    print(f"讀取資料夾: {img_dir} 中的 {len(img_files)} 個檔案")

    for img_file, mask_file in zip(img_files, mask_files):
        # 讀取圖像和遮罩
        img_path = os.path.join(img_dir, img_file)
        mask_path = os.path.join(mask_dir, mask_file)

        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)

        # 調整大小
        if img is not None and mask is not None:
            img = cv2.resize(img, img_size)
            mask = cv2.resize(mask, img_size)

            # 規範化
            img = img / 255.0
            mask = (mask > 0).astype(np.float32)  # 二值化遮罩

            # 添加通道維度
            img = np.expand_dims(img, axis=-1)
            mask = np.expand_dims(mask, axis=-1)

            images.append(img)
            masks.append(mask)
        else:
            print(f"警告: 無法讀取 {img_path} 或 {mask_path}")

    return np.array(images), np.array(masks)

# 資料增強函數 - 極簡版本
def data_augmentation(image, mask):
    """極簡資料增強: 只保留亮度和對比度調整"""
    # 亮度調整
    if np.random.random() > 0.5:
        factor = np.random.uniform(0.8, 1.2)
        image = np.clip(image * factor, 0, 1)

    # 對比度調整
    if np.random.random() > 0.5:
        mean = np.mean(image)
        factor = np.random.uniform(0.8, 1.2)
        image = np.clip((image - mean) * factor + mean, 0, 1)

    return image, mask

# 圖像資料產生器
class ImageDataGenerator(keras.utils.Sequence):
    def __init__(self, images, masks, batch_size=8, augment=False):
        self.images = images
        self.masks = masks
        self.batch_size = batch_size
        self.augment = augment
        self.indexes = np.arange(len(images))
        np.random.shuffle(self.indexes)

    def __len__(self):
        return int(np.ceil(len(self.images) / self.batch_size))

    def __getitem__(self, index):
        batch_indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]
        batch_images = np.array([self.images[i] for i in batch_indexes])
        batch_masks = np.array([self.masks[i] for i in batch_indexes])

        if self.augment:
            # 資料增強
            for i in range(len(batch_images)):
                batch_images[i], batch_masks[i] = data_augmentation(batch_images[i], batch_masks[i])

        return batch_images, batch_masks

    def on_epoch_end(self):
        np.random.shuffle(self.indexes)

# 載入資料
print("載入訓練和驗證資料...")
train_images, train_masks = load_images(train_img_path, train_mask_path)
val_images, val_masks = load_images(val_img_path, val_mask_path)
test_images, test_masks = load_images(test_img_path, test_mask_path)

print(f"訓練資料: {train_images.shape} - {train_masks.shape}")
print(f"驗證資料: {val_images.shape} - {val_masks.shape}")
print(f"測試資料: {test_images.shape} - {test_masks.shape}")

# 建立資料產生器
train_gen = ImageDataGenerator(train_images, train_masks, batch_size=8, augment=True)
val_gen = ImageDataGenerator(val_images, val_masks, batch_size=8, augment=False)

# 自定義端點檢測和評估指標函數
def find_endpoint(mask, threshold=0.5):
    """找出遮罩中的ETT端點（最下方的點）"""
    if np.sum(mask) == 0:  # 如果遮罩為空
        return None

    # 二值化遮罩
    binary_mask = mask > threshold

    # 找出遮罩中的所有點
    points = np.where(binary_mask)
    if len(points[0]) == 0:
        return None

    # 找出y座標最大的點（最下方的點）
    endpoint_idx = np.argmax(points[0])
    y_endpoint = points[0][endpoint_idx]

    return y_endpoint

def mean_error_cm(y_true, y_pred, pixel_per_cm=72):
    """計算平均誤差公分"""
    total_error = 0
    count = 0

    for i in range(len(y_true)):
        true_endpoint = find_endpoint(y_true[i,:,:,0])
        pred_endpoint = find_endpoint(y_pred[i,:,:,0] > 0.5)

        if true_endpoint is not None and pred_endpoint is not None:
            # 計算像素誤差
            error_pixels = abs(true_endpoint - pred_endpoint)
            # 轉換為公分
            error_cm = error_pixels / pixel_per_cm
            total_error += error_cm
            count += 1

    if count == 0:
        return float('inf')

    return total_error / count

def accuracy_within_threshold(y_true, y_pred, threshold_cm, pixel_per_cm=72):
    """計算誤差在threshold_cm公分內的準確率"""
    threshold_pixels = threshold_cm * pixel_per_cm
    correct_count = 0
    total_count = 0

    for i in range(len(y_true)):
        true_endpoint = find_endpoint(y_true[i,:,:,0])
        pred_endpoint = find_endpoint(y_pred[i,:,:,0] > 0.5)

        if true_endpoint is not None and pred_endpoint is not None:
            # 計算像素誤差
            error_pixels = abs(true_endpoint - pred_endpoint)
            if error_pixels <= threshold_pixels:
                correct_count += 1
            total_count += 1

    if total_count == 0:
        return 0.0

    return (correct_count / total_count) * 100

# 自定義IoU指標
def iou_score(y_true, y_pred, smooth=1e-6, threshold=0.5):
    # 確保預測和真實值具有相同的形狀
    y_pred = tf.image.resize(y_pred, tf.shape(y_true)[1:3])
    y_pred_binary = tf.cast(y_pred > threshold, tf.float32)
    intersection = tf.reduce_sum(y_true * y_pred_binary, axis=[1, 2, 3])
    union = tf.reduce_sum(y_true, axis=[1, 2, 3]) + tf.reduce_sum(y_pred_binary, axis=[1, 2, 3]) - intersection
    iou = (intersection + smooth) / (union + smooth)
    return tf.reduce_mean(iou)

# 自定義損失函數
def dice_loss(y_true, y_pred, smooth=1e-6):
    # 確保預測和真實值具有相同的形狀
    y_pred = tf.image.resize(y_pred, tf.shape(y_true)[1:3])
    y_true_f = tf.reshape(y_true, [-1])
    y_pred_f = tf.reshape(y_pred, [-1])
    intersection = tf.reduce_sum(y_true_f * y_pred_f)
    return 1 - (2. * intersection + smooth) / (tf.reduce_sum(y_true_f) + tf.reduce_sum(y_pred_f) + smooth)

# 自定義Focal Loss
def binary_focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):
    # 確保預測和真實值具有相同的形狀
    y_pred = tf.image.resize(y_pred, tf.shape(y_true)[1:3])
    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)
    loss = - (alpha * y_true * tf.pow(1 - y_pred, gamma) * tf.math.log(y_pred) +
             (1 - alpha) * (1 - y_true) * tf.pow(y_pred, gamma) * tf.math.log(1 - y_pred))
    return tf.reduce_mean(loss)

# 修正的端點損失函數 - 使用張量操作而非迴圈
def endpoint_loss(y_true, y_pred, pixel_per_cm=72, weight=1.0):
    """使用張量操作而非迴圈的端點損失函數"""
    # 將預測和真實值轉換為二值遮罩
    y_true_binary = tf.cast(y_true > 0.5, tf.float32)
    y_pred_binary = tf.cast(y_pred > 0.5, tf.float32)

    # 創建一個y軸位置張量
    height = tf.shape(y_true)[1]
    y_coords = tf.cast(tf.range(height), tf.float32)
    y_coords = tf.reshape(y_coords, [1, height, 1, 1])

    # 使用y座標加權遮罩
    y_true_weighted = y_true_binary * y_coords
    y_pred_weighted = y_pred_binary * y_coords

    # 找出每個樣本的最大y值（代表端點位置）
    true_endpoints = tf.reduce_max(y_true_weighted, axis=[1, 2, 3])
    pred_endpoints = tf.reduce_max(y_pred_weighted, axis=[1, 2, 3])

    # 計算端點距離的平方誤差
    endpoint_squared_error = tf.square(true_endpoints - pred_endpoints)

    # 標記有效樣本（同時有真實遮罩和預測遮罩的樣本）
    valid_samples = tf.logical_and(
        tf.reduce_sum(y_true_binary, axis=[1, 2, 3]) > 0,
        tf.reduce_sum(y_pred_binary, axis=[1, 2, 3]) > 0
    )
    valid_samples = tf.cast(valid_samples, tf.float32)

    # 只計算有效樣本的損失
    endpoint_loss = endpoint_squared_error * valid_samples

    # 計算平均損失，避免除以零
    num_valid = tf.maximum(tf.reduce_sum(valid_samples), 1.0)
    mean_endpoint_loss = tf.reduce_sum(endpoint_loss) / num_valid

    return weight * mean_endpoint_loss

# 組合損失函數
def combined_loss(y_true, y_pred):
    """組合多種損失函數以獲得最佳效果"""
    dice = dice_loss(y_true, y_pred)
    focal = binary_focal_loss(y_true, y_pred)
    endpoint = endpoint_loss(y_true, y_pred)

    # 平衡各損失比重
    return 0.6 * dice + 0.3 * focal + 0.1 * endpoint

# 自定義評估回調函數
class ETTEndpointMetrics(tf.keras.callbacks.Callback):
    def __init__(self, validation_data, pixel_per_cm=72):
        super(ETTEndpointMetrics, self).__init__()
        self.validation_data = validation_data
        self.pixel_per_cm = pixel_per_cm
        self.metrics_history = {
            'val_mean_error_cm': [],
            'val_accuracy_0.5cm': [],
            'val_accuracy_1.0cm': []
        }

    def on_epoch_end(self, epoch, logs=None):
        # 獲取驗證資料
        val_images, val_masks = self.validation_data

        # 預測
        val_preds = self.model.predict(val_images)

        # 計算端點指標
        mean_err = mean_error_cm(val_masks, val_preds, self.pixel_per_cm)
        acc_0_5 = accuracy_within_threshold(val_masks, val_preds, 0.5, self.pixel_per_cm)
        acc_1_0 = accuracy_within_threshold(val_masks, val_preds, 1.0, self.pixel_per_cm)

        # 更新logs
        logs = logs or {}
        logs['val_mean_error_cm'] = mean_err
        logs['val_accuracy_0.5cm'] = acc_0_5
        logs['val_accuracy_1.0cm'] = acc_1_0

        # 儲存指標歷史
        self.metrics_history['val_mean_error_cm'].append(mean_err)
        self.metrics_history['val_accuracy_0.5cm'].append(acc_0_5)
        self.metrics_history['val_accuracy_1.0cm'].append(acc_1_0)

        # 輸出指標
        print(f" - val_mean_error_cm: {mean_err:.4f} - val_accuracy_0.5cm: {acc_0_5:.2f}% - val_accuracy_1.0cm: {acc_1_0:.2f}%")

# 建立 Atrous Spatial Pyramid Pooling (ASPP) 模塊
def ASPP(inputs, dilations=[6, 12, 18]):
    # 圖像輸入的形狀
    _, h, w, c = inputs.shape

    # 全局平均池化分支
    global_avg_pool = layers.GlobalAveragePooling2D()(inputs)
    global_avg_pool = layers.Reshape((1, 1, c))(global_avg_pool)
    global_avg_pool = layers.Conv2D(256, kernel_size=1, padding='same', use_bias=False)(global_avg_pool)
    global_avg_pool = layers.BatchNormalization()(global_avg_pool)
    global_avg_pool = layers.Activation('relu')(global_avg_pool)
    global_avg_pool = layers.UpSampling2D(size=(h, w), interpolation='bilinear')(global_avg_pool)

    # 1x1 卷積分支
    conv_1x1 = layers.Conv2D(256, kernel_size=1, padding='same', use_bias=False)(inputs)
    conv_1x1 = layers.BatchNormalization()(conv_1x1)
    conv_1x1 = layers.Activation('relu')(conv_1x1)

    # 多個擴張卷積分支
    atrous_branches = []
    for dilation_rate in dilations:
        x = layers.Conv2D(256, kernel_size=3, padding='same',
                          dilation_rate=dilation_rate, use_bias=False)(inputs)
        x = layers.BatchNormalization()(x)
        x = layers.Activation('relu')(x)
        atrous_branches.append(x)

    # 合併所有分支
    concat_branches = [conv_1x1, global_avg_pool] + atrous_branches
    x = layers.Concatenate(axis=-1)(concat_branches)

    # 輸出投影
    x = layers.Conv2D(256, kernel_size=1, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    return x

# 建立DeepLabV3+模型
def build_deeplabv3_plus_model(input_shape=(256, 256, 1), dilations=[6, 12, 18]):
    # 輸入層
    inputs = keras.Input(shape=input_shape)

    # 編碼器部分 - 使用更深的ResNet架構
    # 初始處理
    x = layers.Conv2D(64, 7, strides=2, padding='same')(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.MaxPooling2D(3, strides=2, padding='same')(x)

    # 保存低層特徵用於decoder的skip connection
    low_level_features_32 = x  # 64x64

    # ResNet塊 1
    skip_connection = x
    x = layers.Conv2D(64, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2D(64, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, skip_connection])
    x = layers.Activation('relu')(x)

    # ResNet塊 2 - 下採樣
    skip_connection = layers.Conv2D(128, 1, strides=2, padding='same')(x)
    skip_connection = layers.BatchNormalization()(skip_connection)

    x = layers.Conv2D(128, 3, strides=2, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2D(128, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, skip_connection])
    x = layers.Activation('relu')(x)

    # 保存低層特徵用於decoder
    low_level_features = x  # 32x32

    # ResNet塊 3 - 下採樣
    skip_connection = layers.Conv2D(256, 1, strides=2, padding='same')(x)
    skip_connection = layers.BatchNormalization()(skip_connection)

    x = layers.Conv2D(256, 3, strides=2, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2D(256, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, skip_connection])
    x = layers.Activation('relu')(x)

    # ResNet塊 4 - 下採樣
    skip_connection = layers.Conv2D(512, 1, strides=2, padding='same')(x)
    skip_connection = layers.BatchNormalization()(skip_connection)

    x = layers.Conv2D(512, 3, strides=2, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)
    x = layers.Conv2D(512, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Add()([x, skip_connection])
    x = layers.Activation('relu')(x)

    # ASPP模塊
    x = ASPP(x, dilations=dilations)

    # 解碼器部分 - 多層級特徵融合
    # 第一次上採樣到32x32
    x = layers.UpSampling2D(size=(4, 4), interpolation='bilinear')(x)

    # 處理低層特徵
    low_level_features = layers.Conv2D(48, 1, padding='same', use_bias=False)(low_level_features)
    low_level_features = layers.BatchNormalization()(low_level_features)
    low_level_features = layers.Activation('relu')(low_level_features)

    # 特徵融合
    x = layers.Concatenate()([x, low_level_features])

    # 3x3卷積提煉特徵
    x = layers.Conv2D(256, 3, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    x = layers.Conv2D(256, 3, padding='same', use_bias=False)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    # 第二層特徵融合
    x = layers.UpSampling2D(size=(2, 2), interpolation='bilinear')(x)  # 64x64

    # 處理最低層特徵
    low_level_features_32 = layers.Conv2D(32, 1, padding='same', use_bias=False)(low_level_features_32)
    low_level_features_32 = layers.BatchNormalization()(low_level_features_32)
    low_level_features_32 = layers.Activation('relu')(low_level_features_32)

    # 特徵融合
    x = layers.Concatenate()([x, low_level_features_32])

    # 最終上採樣到256x256
    x = layers.UpSampling2D(size=(4, 4), interpolation='bilinear')(x)  # 256x256

    # 最終特徵提煉
    x = layers.Conv2D(128, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    x = layers.Conv2D(64, 3, padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.Activation('relu')(x)

    # 輸出層 - 1個通道用於二值分割
    outputs = layers.Conv2D(1, 1, activation='sigmoid')(x)

    # 建立模型
    model = keras.Model(inputs, outputs)

    # 編譯模型
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-4),
        loss=combined_loss,
        metrics=[iou_score]
    )

    return model

# 建立模型
model = build_deeplabv3_plus_model(input_shape=(256, 256, 1), dilations=[6, 12, 18])
model.summary()

# 設定回調函數
checkpoint = ModelCheckpoint(
    '/content/results/fold3/best_model.h5',
    monitor='val_iou_score',
    mode='max',
    save_best_only=True,
    verbose=1
)

early_stopping = EarlyStopping(
    monitor='val_iou_score',
    mode='max',
    patience=15,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_iou_score',
    mode='max',
    factor=0.5,
    patience=5,
    min_lr=1e-6,
    verbose=1
)

tensorboard = TensorBoard(
    log_dir='/content/results/fold3/logs',
    write_images=True
)

endpoint_metrics = ETTEndpointMetrics(
    validation_data=(val_images, val_masks),
    pixel_per_cm=72
)

# 訓練模型
print("開始訓練模型...")
EPOCHS = 20
BATCH_SIZE = 8

history = model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=EPOCHS,
    callbacks=[checkpoint, early_stopping, reduce_lr, tensorboard, endpoint_metrics],
    verbose=1
)

# 載入最佳模型
best_model = keras.models.load_model(
    '/content/results/fold3/best_model.h5',
    custom_objects={
        'iou_score': iou_score,
        'combined_loss': combined_loss,
        'dice_loss': dice_loss,
        'binary_focal_loss': binary_focal_loss,
        'endpoint_loss': endpoint_loss
    }
)

# 評估模型
print("評估測試集...")
test_preds = best_model.predict(test_images)
test_iou = iou_score(test_masks, test_preds).numpy()
test_mean_error = mean_error_cm(test_masks, test_preds)
test_acc_0_5 = accuracy_within_threshold(test_masks, test_preds, 0.5)
test_acc_1_0 = accuracy_within_threshold(test_masks, test_preds, 1.0)

# 儲存評估結果
results = {
    'fold': 3,
    'iou_score': float(test_iou),
    'mean_error_cm': test_mean_error,
    'accuracy_0.5cm': test_acc_0_5,
    'accuracy_1.0cm': test_acc_1_0
}

# 將結果轉換為DataFrame並儲存為CSV
results_df = pd.DataFrame([results])
results_df.to_csv('/content/results/fold3/metrics.csv', index=False)
print("測試集評估結果:")
print(f"IOU Score: {test_iou:.4f}")
print(f"Mean Error (cm): {test_mean_error:.4f}")
print(f"Accuracy within 0.5cm: {test_acc_0_5:.2f}%")
print(f"Accuracy within 1.0cm: {test_acc_1_0:.2f}%")

# 視覺化訓練歷史
plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.plot(history.history['iou_score'])
plt.plot(history.history['val_iou_score'])
plt.title('模型 IoU 分數')
plt.ylabel('IoU')
plt.xlabel('訓練輪次')
plt.legend(['訓練集', '驗證集'], loc='upper left')

plt.subplot(1, 3, 2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('模型損失')
plt.ylabel('損失')
plt.xlabel('訓練輪次')
plt.legend(['訓練集', '驗證集'], loc='upper left')

plt.subplot(1, 3, 3)
plt.plot(endpoint_metrics.metrics_history['val_mean_error_cm'])
plt.title('平均誤差 (公分)')
plt.ylabel('誤差')
plt.xlabel('訓練輪次')
plt.legend(['驗證集'], loc='upper left')

plt.savefig('/content/results/fold3/training_history.png')
plt.show()

# 視覺化預測結果
def visualize_results(images, masks, predictions, num_samples=5):
    fig, axes = plt.subplots(num_samples, 3, figsize=(15, num_samples*5))

    for i in range(min(num_samples, len(images))):
        # 原始圖像
        axes[i, 0].imshow(images[i, :, :, 0], cmap='gray')
        axes[i, 0].set_title('原始圖像')
        axes[i, 0].axis('off')

        # 真實遮罩
        axes[i, 1].imshow(masks[i, :, :, 0], cmap='viridis')
        true_endpoint = find_endpoint(masks[i, :, :, 0])
        if true_endpoint is not None:
            axes[i, 1].scatter(images.shape[2]//2, true_endpoint, c='r', marker='x', s=100)
            axes[i, 1].text(images.shape[2]//2, true_endpoint+10, 'G', color='red', fontsize=12)
        axes[i, 1].set_title('真實遮罩')
        axes[i, 1].axis('off')

        # 預測遮罩
        axes[i, 2].imshow(predictions[i, :, :, 0], cmap='viridis')
        pred_endpoint = find_endpoint(predictions[i, :, :, 0] > 0.5)
        if pred_endpoint is not None:
            axes[i, 2].scatter(images.shape[2]//2, pred_endpoint, c='y', marker='x', s=100)
            axes[i, 2].text(images.shape[2]//2, pred_endpoint+10, 'Y', color='yellow', fontsize=12)
        axes[i, 2].set_title('預測遮罩')
        axes[i, 2].axis('off')

        # 添加誤差資訊
        if true_endpoint is not None and pred_endpoint is not None:
            error_pixels = abs(true_endpoint - pred_endpoint)
            error_cm = error_pixels / 72.0
            axes[i, 0].set_title(f'誤差: {error_cm:.2f} 公分')

    plt.tight_layout()
    plt.savefig('/content/results/fold3/predictions.png')
    plt.show()

# 視覺化測試集結果
print("視覺化預測結果...")
sample_indices = np.random.choice(len(test_images), size=min(5, len(test_images)), replace=False)
visualize_results(
    test_images[sample_indices],
    test_masks[sample_indices],
    test_preds[sample_indices]
)

# 壓縮並下載結果
!zip -r /content/fold3_results.zip /content/results/fold3
files.download('/content/fold3_results.zip')

print("Fold3訓練完成！")

